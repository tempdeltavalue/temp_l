{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225b0a06-4dd3-4c84-89ed-86de4adcd24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tempdelta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/peft/index\n",
    "\n",
    "import os \n",
    "\n",
    "### Utils\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def split_strings(strings):\n",
    "    first_parts = []\n",
    "    second_parts = []\n",
    "    for string in strings:\n",
    "        split_index = len(string) // 2\n",
    "        while split_index < len(string) and not string[split_index].isspace():\n",
    "            split_index += 1\n",
    "        first_part = string[:split_index].rstrip()\n",
    "        second_part = string[split_index:].lstrip()\n",
    "        first_parts.append(first_part)\n",
    "        second_parts.append(second_part)\n",
    "    return first_parts, second_parts\n",
    "    \n",
    "### \n",
    "\n",
    "math_example_path = os.getcwd() + '/data/What_Is_Mathematics_An_Elementary_Approach_to_Ideas_and_Methods.txt'\n",
    "\n",
    "with open(math_example_path, \"r\",  encoding=\"utf8\") as f:\n",
    "     math_example_text = f.read()\n",
    "\n",
    "math_example_text = math_example_text.replace(\"\\n\", \"\")\n",
    "math_sentences = math_example_text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28fa07d-ee0a-44cb-b5f0-f14226be3244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9421\n",
      " But it is not a concession to the dangerous tendency toward dodging all exertion\n"
     ]
    }
   ],
   "source": [
    "print(len(math_sentences))\n",
    "math_sentences = [item for item in math_sentences if 80 < len(item) < 100]\n",
    "X, Y = split_strings(math_sentences)\n",
    "print(math_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f996a8d-7395-419c-94be-75f2c6d80dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For a « 20, we need twenty words for the digits, plus words for 20 and 400, making a total of 22 \n",
      " X:   For a « 20, we need twenty words for the digits, \n",
      " Y:  plus words for 20 and 400, making a total of 22\n"
     ]
    }
   ],
   "source": [
    "tst_ind = 30\n",
    "print(math_sentences[tst_ind], \"\\n X: \", X[tst_ind], \"\\n Y: \", Y[tst_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c1921fe-4e0e-43d8-aef3-07e307f890d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"gpt2\" #\"./finetuned_llm_all\" #\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left', add_eos_token=True) #gpt2\n",
    "llm_model = GPT2LMHeadModel.from_pretrained(model_name) # gpt2\n",
    "device = torch.device('cuda')\n",
    "llm_model.to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configure the model\n",
    "llm_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "llm_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "llm_model.config.vocab_size = llm_model.config.vocab_size + len(tokenizer.get_added_vocab())\n",
    "llm_model.resize_token_embeddings(len(tokenizer))\n",
    "llm_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f76d6675-bfda-4499-887a-4db95eed1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(309)\n",
    "\n",
    "# Set padding to the left side as the model is a decoder-only architecture\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "encoded_X = tokenizer.batch_encode_plus(X, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "encoded_Y = tokenizer.batch_encode_plus(Y, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "\n",
    "BATH_SIZE = 10\n",
    "\n",
    "x_batches = list(chunks(encoded_X[\"input_ids\"], BATH_SIZE))\n",
    "y_batches = list(chunks(encoded_Y[\"input_ids\"], BATH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2e4e4-6cc3-49ff-a68f-9ba24585de7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e255de3-0840-4b1a-99a3-9d2e72887742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data\n",
    "test_ids = np.random.choice(range(len(X)), 2)\n",
    "test_X = [X[i] for i in test_ids]\n",
    "encoded_test_X = tokenizer.batch_encode_plus(test_X, \n",
    "                                     add_special_tokens=False, \n",
    "                                     return_tensors='pt', \n",
    "                                     padding=True) # X is encoded Y isnt !\n",
    "\n",
    "\n",
    "test_Y = [Y[i] for i in test_ids]\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ceed72ab-e4ee-4ad0-8b64-3596886377dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3914,   514,  2074,   262, 20128,   286,   257,  6614])\n",
      " Let us consider the projection of a plane\n"
     ]
    }
   ],
   "source": [
    "test_index = 0\n",
    "test_item = encoded_test_X[\"input_ids\"][test_index]\n",
    "print(test_item)\n",
    "print(tokenizer.decode(test_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6edb8f0-fcc5-430f-b462-8ab6fd42aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_model.generate(encoded_test_X[\"input_ids\"].to(device),  max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7978100-e9ba-4d65-a21a-9e509e9f12df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3914,   514,  2074,   262, 20128,   286,   257,  6614,   284,   262,\n",
      "        17810,    13,   198,   198,   464, 20128,   286,   257,  6614,   284,\n",
      "          262, 17810,   318,   262, 20128,   286,   257,  6614,   284,   262,\n",
      "        17810,    13,   383, 20128,   286,   257,  6614,   284,   262, 17810,\n",
      "          318,   262, 20128,   286,   257,  6614,   284,   262, 17810,    13,\n",
      "          383, 20128,   286,   257,  6614,   284,   262, 17810,   318,   262,\n",
      "        20128,   286,   257,  6614,   284,   262, 17810,    13,   383, 20128,\n",
      "          286,   257,  6614,   284,   262, 17810,   318,   262, 20128,   286,\n",
      "          257,  6614,   284,   262, 17810,    13,   383, 20128,   286,   257,\n",
      "         6614,   284,   262, 17810,   318,   262, 20128,   286,   257,  6614,\n",
      "          284,   262, 17810,    13,   383, 20128,   286,   257,  6614,   284,\n",
      "          262, 17810,   318,   262, 20128,   286,   257,  6614,   284,   262,\n",
      "        17810,    13,   383, 20128,   286,   257,  6614,   284,   262, 17810,\n",
      "          318,   262, 20128,   286,   257,  6614,   284,   262, 17810,    13,\n",
      "          383, 20128,   286,   257,  6614,   284,   262, 17810,   318,   262,\n",
      "        20128,   286,   257,  6614,   284,   262, 17810,    13,   383, 20128,\n",
      "          286,   257,  6614,   284,   262, 17810,   318,   262, 20128,   286,\n",
      "          257,  6614,   284,   262, 17810,    13,   383, 20128,   286,   257,\n",
      "         6614,   284,   262, 17810,   318,   262, 20128,   286,   257,  6614,\n",
      "          284,   262, 17810,    13,   383, 20128,   286,   257,  6614,   284,\n",
      "          262, 17810,   318,   262, 20128,   286,   257,  6614],\n",
      "       device='cuda:0') \n",
      " \n",
      "output : \n",
      "  Let us consider the projection of a plane to the horizon.\n",
      "\n",
      "The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane to the horizon. The projection of a plane to the horizon is the projection of a plane\n"
     ]
    }
   ],
   "source": [
    "print(output[test_index], \"\\n \")\n",
    "print(\"output : \\n\", tokenizer.decode(output[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33487168-8fbb-4b64-884e-e1eaaaede66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "\n",
    "tst_i = 3\n",
    "test_str = test_X[tst_i]\n",
    "input_ids = tokenizer.encode(test_str,\n",
    "                             add_special_tokens=False, \n",
    "                             return_tensors='pt', \n",
    "                             padding=True).to(device)\n",
    "\n",
    "# Generate text using generation parameters\n",
    "output = llm_model.generate(input_ids , \n",
    "                            max_new_tokens=100, \n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(output[0])\n",
    "print(f\"input: {test_str} \\n\")\n",
    "print(f\"input_ids: {input_ids} \\n\")\n",
    "print(f\"output: {output} \\n\")\n",
    "\n",
    "print(f\"decoded_output:{decoded_output}\\n\", )\n",
    "print(f\"expected output:{test_Y[tst_i]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25b5ed-50c6-4337-8134-2bff8862908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(llm_model.parameters(), lr=1e-5)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbcac2-39c4-4ec3-9f7c-3ea27a109688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "EPOCHS=10\n",
    "\n",
    "mlflow.end_run()\n",
    "gradient_accumulation_steps = 10\n",
    "grad_counter = 0\n",
    "\n",
    "with mlflow.start_run(run_name='final') as run:\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for index, batch in enumerate(x_batches[0:5]): \n",
    "            outputs = llm_model(batch, labels=y_batches[index])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_counter % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                print(\"batch loss \", loss.item())\n",
    "\n",
    "                \n",
    "            grad_counter += 1\n",
    "                            \n",
    "\n",
    "        # log metric every epoch\n",
    "        mlflow.log_metric('loss', loss.item())\n",
    "        mlflow.log_metric('lr', get_lr(optimizer))\n",
    "\n",
    "        \n",
    "        output = llm_model.generate(tst_data[\"input_ids\"].to(device))\n",
    "        test_output = \"\"\n",
    "        for i in range(output.shape[0]):\n",
    "            tmp_str = tokenizer.decode(output[i])\n",
    "            test_output += \"\\n\" + tmp_str\n",
    "        mlflow.log_text(test_output, f\"test_{epoch}.txt\")\n",
    "        #### \n",
    "\n",
    "        \n",
    "        # Save the fine-tuned model every epoch \n",
    "        print(\"MODEL saved loss \", loss)\n",
    "        llm_model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89cbc93-1cd3-463e-9bb3-472f3dbe7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tst_text = np.random.choice(math_sentences, 30)\n",
    "tst_data = tokenizer.batch_encode_plus(tst_text, add_special_tokens=True, return_tensors='pt', padding=True).to(device)\n",
    "output = llm_model.generate(tst_data[\"input_ids\"])\n",
    "\n",
    "for i, text in enumerate(tst_text):\n",
    "    print(\"input:\", text)\n",
    "    print(\"output\", tokenizer.decode(output[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe6a06-f620-4ed7-9537-026702defb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "      tokenizer=tokenizer,\n",
    "      file_path=math_example_path,\n",
    "      block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6078a30-e05d-447f-9ca4-e0dcd4c38fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    default_data_collator, \n",
    ")\n",
    "import mlflow \n",
    "mlflow.end_run()\n",
    "\n",
    "training_args = TrainingArguments(output_dir='test_trainer', \n",
    "                                  #evaluation_strategy='epoch',\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  gradient_accumulation_steps=20, #\n",
    "                                  num_train_epochs = 2,\n",
    "                                  fp16=False,)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=llm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    #eval_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=None,\n",
    "    preprocess_logits_for_metrics=None,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027628bf-339b-41ad-992a-43f5145a5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381e7ac-d923-47ab-98b5-b355409bf892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
