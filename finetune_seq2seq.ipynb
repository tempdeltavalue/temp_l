{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225b0a06-4dd3-4c84-89ed-86de4adcd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/peft/index\n",
    "\n",
    "import os \n",
    "\n",
    "### Utils\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def split_strings(strings):\n",
    "    first_parts = []\n",
    "    second_parts = []\n",
    "    for string in strings:\n",
    "        split_index = len(string) // 2\n",
    "        while split_index < len(string) and not string[split_index].isspace():\n",
    "            split_index += 1\n",
    "        first_part = string[:split_index].rstrip()\n",
    "        second_part = string[split_index:].lstrip()\n",
    "        first_parts.append(first_part)\n",
    "        second_parts.append(second_part)\n",
    "    return first_parts, second_parts\n",
    "    \n",
    "### \n",
    "\n",
    "math_example_path = os.getcwd() + '/data/What_Is_Mathematics_An_Elementary_Approach_to_Ideas_and_Methods.txt'\n",
    "\n",
    "with open(math_example_path, \"r\",  encoding=\"utf8\") as f:\n",
    "     math_example_text = f.read()\n",
    "\n",
    "math_example_text = math_example_text.replace(\"\\n\", \"\")\n",
    "math_sentences = math_example_text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28fa07d-ee0a-44cb-b5f0-f14226be3244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9421\n",
      " But it is not a concession to the dangerous tendency toward dodging all exertion\n"
     ]
    }
   ],
   "source": [
    "print(len(math_sentences))\n",
    "math_sentences = [item for item in math_sentences if 80 < len(item) < 100]\n",
    "X, Y = split_strings(math_sentences)\n",
    "print(math_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f996a8d-7395-419c-94be-75f2c6d80dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For a « 20, we need twenty words for the digits, plus words for 20 and 400, making a total of 22 \n",
      " X:   For a « 20, we need twenty words for the digits, \n",
      " Y:  plus words for 20 and 400, making a total of 22\n"
     ]
    }
   ],
   "source": [
    "tst_ind = 30\n",
    "print(math_sentences[tst_ind], \"\\n X: \", X[tst_ind], \"\\n Y: \", Y[tst_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1921fe-4e0e-43d8-aef3-07e307f890d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tempdelta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\tempdelta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# for step in range(5):\n",
    "#     input_str = input(\">> User:\") + tokenizer.eos_token\n",
    "#     new_user_input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
    "\n",
    "#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1).to(device) if step > 0 else new_user_input_ids\n",
    "\n",
    "#     chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "\n",
    "#     print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2b0231-98d6-46c0-8644-d6de40ad585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  887,   340,   318,   407,   257, 33130,   284,   262,  4923, 50256,\n",
      "        50256])\n",
      " But it is not a concession to the dangerous<|endoftext|><|endoftext|>\n",
      "tensor([  887,   340,   318,   407,   257, 33130,   284,   262,  4923, 50256,\n",
      "        50256,    40,  1101,   407,  1654,   644,   345,   821,  2111,   284,\n",
      "          910,   764, 50256], device='cuda:0') \n",
      " \n",
      " But it is not a concession to the dangerousI'm not sure what you're trying to say.\n"
     ]
    }
   ],
   "source": [
    "def vis_res(str_Xs, vis_i = 0):\n",
    "    encoded_test_X = tokenizer.batch_encode_plus(str_Xs, \n",
    "                                         add_special_tokens=False, \n",
    "                                         return_tensors='pt', \n",
    "                                         padding=True) # X is encoded Y isnt !\n",
    "    \n",
    "    ###\n",
    "    test_item = encoded_test_X[\"input_ids\"][vis_i]\n",
    "    print(test_item)\n",
    "    print(tokenizer.decode(test_item))\n",
    "    \n",
    "    output = model.generate(encoded_test_X[\"input_ids\"].to(device),  \n",
    "                                max_new_tokens=200,   \n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    print(output[vis_i], \"\\n \")\n",
    "    print(tokenizer.decode(output[vis_i], skip_special_tokens=True))\n",
    "\n",
    "#vis_res([\"Do you love me?\" + tokenizer.eos_token])\n",
    "vis_res([\" But it is not a concession to the dangerous<|endoftext|>\" + tokenizer.eos_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315e9e25-a4d6-445e-8851-836556dcc036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be called only once\n",
    "is_called = False\n",
    "\n",
    "if is_called is False:\n",
    "    X = [s + tokenizer.eos_token for s in X]\n",
    "    Y = [s + tokenizer.eos_token for s in Y]\n",
    "    is_called = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df43d195-7629-45bf-bc93-947697d8cdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str inp  Unfortunately, professional repre¬ sentatives<|endoftext|>\n",
      "torch t: tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256,  8989,    11,  4708,  1128,   260,   126,   105,\n",
      "         1908,  2929, 50256])\n"
     ]
    }
   ],
   "source": [
    "tt_x = tokenizer(X[0:33], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tt_x[\"input_ids\"] \n",
    "\n",
    "t_inp = torch.Tensor(input_ids)\n",
    "\n",
    "print(\"str inp\", X[0])\n",
    "\n",
    "print(\"torch t:\", t_inp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80fd213-c90a-4321-a55c-3a4796caa79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,   887,\n",
      "          340,   318,   407,   257, 33130,   284,   262,  4923, 50256])\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> But it is not a concession to the dangerous<|endoftext|>\n",
      "tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,   887,\n",
      "          340,   318,   407,   257, 33130,   284,   262,  4923, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256], device='cuda:0') \n",
      " \n",
      " But it is not a concession to the dangerous\n"
     ]
    }
   ],
   "source": [
    "vis_i = 1\n",
    "encoded_test_X = tokenizer.batch_encode_plus(X[0:99], \n",
    "                                     add_special_tokens=False, \n",
    "                                     return_tensors='pt', \n",
    "                                     padding=True) # X is encoded Y isnt !\n",
    "\n",
    "###\n",
    "test_item = encoded_test_X[\"input_ids\"][vis_i]\n",
    "print(test_item)\n",
    "print(tokenizer.decode(test_item))\n",
    "\n",
    "output = model.generate(encoded_test_X[\"input_ids\"].to(device),  \n",
    "                            max_new_tokens=200,   \n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(output[vis_i], \"\\n \")\n",
    "print(tokenizer.decode(output[vis_i], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c084d9-ed24-4d58-967b-dc09fd9ac0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([99, 29])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_X[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d6675-bfda-4499-887a-4db95eed1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(309)\n",
    "\n",
    "# Set padding to the left side as the model is a decoder-only architecture\n",
    "\n",
    "encoded_X = tokenizer.batch_encode_plus(X, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "encoded_Y = tokenizer.batch_encode_plus(Y, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "\n",
    "BATH_SIZE = 10\n",
    "\n",
    "x_batches = list(chunks(encoded_X[\"input_ids\"], BATH_SIZE))\n",
    "y_batches = list(chunks(encoded_Y[\"input_ids\"], BATH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed72ab-e4ee-4ad0-8b64-3596886377dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 0\n",
    "test_item = encoded_X[\"input_ids\"][test_index]\n",
    "\n",
    "print(X[test_index])\n",
    "print(test_item)\n",
    "print(tokenizer.decode(test_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edb8f0-fcc5-430f-b462-8ab6fd42aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(x_batches[0].to(device),  max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7978100-e9ba-4d65-a21a-9e509e9f12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[test_index], \"\\n \")\n",
    "print(tokenizer.decode(output[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33487168-8fbb-4b64-884e-e1eaaaede66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "\n",
    "tst_i = 3\n",
    "test_str = test_X[tst_i]\n",
    "input_ids = tokenizer.encode(test_str,\n",
    "                             add_special_tokens=False, \n",
    "                             return_tensors='pt', \n",
    "                             padding=True).to(device)\n",
    "\n",
    "# Generate text using generation parameters\n",
    "output = llm_model.generate(input_ids , \n",
    "                            max_new_tokens=100, \n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(output[0])\n",
    "print(f\"input: {test_str} \\n\")\n",
    "print(f\"input_ids: {input_ids} \\n\")\n",
    "print(f\"output: {output} \\n\")\n",
    "\n",
    "print(f\"decoded_output:{decoded_output}\\n\", )\n",
    "print(f\"expected output:{test_Y[tst_i]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25b5ed-50c6-4337-8134-2bff8862908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(llm_model.parameters(), lr=1e-5)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbcac2-39c4-4ec3-9f7c-3ea27a109688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "EPOCHS=10\n",
    "\n",
    "mlflow.end_run()\n",
    "gradient_accumulation_steps = 10\n",
    "grad_counter = 0\n",
    "\n",
    "with mlflow.start_run(run_name='final') as run:\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for index, batch in enumerate(x_batches[0:5]): \n",
    "            outputs = llm_model(batch, labels=y_batches[index])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_counter % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                print(\"batch loss \", loss.item())\n",
    "\n",
    "                \n",
    "            grad_counter += 1\n",
    "                            \n",
    "\n",
    "        # log metric every epoch\n",
    "        mlflow.log_metric('loss', loss.item())\n",
    "        mlflow.log_metric('lr', get_lr(optimizer))\n",
    "\n",
    "        \n",
    "        output = llm_model.generate(tst_data[\"input_ids\"].to(device))\n",
    "        test_output = \"\"\n",
    "        for i in range(output.shape[0]):\n",
    "            tmp_str = tokenizer.decode(output[i])\n",
    "            test_output += \"\\n\" + tmp_str\n",
    "        mlflow.log_text(test_output, f\"test_{epoch}.txt\")\n",
    "        #### \n",
    "\n",
    "        \n",
    "        # Save the fine-tuned model every epoch \n",
    "        print(\"MODEL saved loss \", loss)\n",
    "        llm_model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89cbc93-1cd3-463e-9bb3-472f3dbe7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tst_text = np.random.choice(math_sentences, 30)\n",
    "tst_data = tokenizer.batch_encode_plus(tst_text, add_special_tokens=True, return_tensors='pt', padding=True).to(device)\n",
    "output = llm_model.generate(tst_data[\"input_ids\"])\n",
    "\n",
    "for i, text in enumerate(tst_text):\n",
    "    print(\"input:\", text)\n",
    "    print(\"output\", tokenizer.decode(output[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe6a06-f620-4ed7-9537-026702defb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "      tokenizer=tokenizer,\n",
    "      file_path=math_example_path,\n",
    "      block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6078a30-e05d-447f-9ca4-e0dcd4c38fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    default_data_collator, \n",
    ")\n",
    "import mlflow \n",
    "mlflow.end_run()\n",
    "\n",
    "training_args = TrainingArguments(output_dir='test_trainer', \n",
    "                                  #evaluation_strategy='epoch',\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  gradient_accumulation_steps=20, #\n",
    "                                  num_train_epochs = 2,\n",
    "                                  fp16=False,)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=llm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    #eval_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=None,\n",
    "    preprocess_logits_for_metrics=None,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027628bf-339b-41ad-992a-43f5145a5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381e7ac-d923-47ab-98b5-b355409bf892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
