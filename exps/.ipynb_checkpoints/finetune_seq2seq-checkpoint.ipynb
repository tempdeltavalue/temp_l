{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225b0a06-4dd3-4c84-89ed-86de4adcd24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tempdelta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/peft/index\n",
    "\n",
    "import os \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "### Utils\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def split_strings(strings):\n",
    "    first_parts = []\n",
    "    second_parts = []\n",
    "    for string in strings:\n",
    "        split_index = len(string) // 2\n",
    "        while split_index < len(string) and not string[split_index].isspace():\n",
    "            split_index += 1\n",
    "        first_part = string[:split_index].rstrip()\n",
    "        second_part = string[split_index:].lstrip()\n",
    "        first_parts.append(first_part)\n",
    "        second_parts.append(second_part)\n",
    "    return first_parts, second_parts\n",
    "    \n",
    "### \n",
    "\n",
    "math_example_path = os.getcwd() + '/data/What_Is_Mathematics_An_Elementary_Approach_to_Ideas_and_Methods.txt'\n",
    "\n",
    "with open(math_example_path, \"r\",  encoding=\"utf8\") as f:\n",
    "     math_example_text = f.read()\n",
    "\n",
    "math_example_text = math_example_text.replace(\"\\n\", \"\")\n",
    "math_sentences = math_example_text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28fa07d-ee0a-44cb-b5f0-f14226be3244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9421\n",
      " But it is not a concession to the dangerous tendency toward dodging all exertion\n"
     ]
    }
   ],
   "source": [
    "print(len(math_sentences))\n",
    "math_sentences = [item for item in math_sentences if 80 < len(item) < 100]\n",
    "X, Y = split_strings(math_sentences)\n",
    "print(math_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f996a8d-7395-419c-94be-75f2c6d80dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For a « 20, we need twenty words for the digits, plus words for 20 and 400, making a total of 22 \n",
      " X:   For a « 20, we need twenty words for the digits, \n",
      " Y:  plus words for 20 and 400, making a total of 22\n"
     ]
    }
   ],
   "source": [
    "tst_ind = 30\n",
    "print(math_sentences[tst_ind], \"\\n X: \", X[tst_ind], \"\\n Y: \", Y[tst_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1921fe-4e0e-43d8-aef3-07e307f890d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████| 26.0/26.0 [00:00<00:00, 25.9kB/s]\n",
      "C:\\Users\\tempdelta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tempdelta\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 1.95MB/s]\n",
      "merges.txt: 100%|███████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 3.76MB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 4.31MB/s]\n",
      "config.json: 100%|█████████████████████████████████████████████████████████████████████| 665/665 [00:00<00:00, 666kB/s]\n",
      "model.safetensors: 100%|████████████████████████████████████████████████████████████| 548M/548M [01:42<00:00, 5.33MB/s]\n",
      "generation_config.json: 100%|█████████████████████████████████████████████████████████████████| 124/124 [00:00<?, ?B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "output_dir = \"./finetuned_llm_all\"\n",
    "\n",
    "model_name = \"gpt2\" #output_dir #\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left') #gpt2\n",
    "llm_model = GPT2LMHeadModel.from_pretrained(model_name) # gpt2\n",
    "device = torch.device('cuda')\n",
    "llm_model.to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configure the model\n",
    "llm_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "llm_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "llm_model.config.vocab_size = llm_model.config.vocab_size + len(tokenizer.get_added_vocab())\n",
    "llm_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76d6675-bfda-4499-887a-4db95eed1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len_params = len(list(llm_model.parameters()))\n",
    "#for i, param in enumerate(llm_model.parameters()):\n",
    "#    param.requires_grad = i/len_params > 0.8 # train last 20% of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd2e4e4-6cc3-49ff-a68f-9ba24585de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#np.random.seed(309)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "llm_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Set padding to the left side as the model is a decoder-only architecture\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "encoded_X = tokenizer.batch_encode_plus(X, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "encoded_Y = tokenizer.batch_encode_plus(Y, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "\n",
    "test_ids = np.random.choice(range(len(X)), 5)\n",
    "test_X = [X[i] for i in test_ids]\n",
    "enc_test_X = tokenizer.batch_encode_plus(test_X, \n",
    "                                     add_special_tokens=False, \n",
    "                                     return_tensors='pt', \n",
    "                                     padding=True) # X is encoded Y isnt !\n",
    "\n",
    "\n",
    "test_Y = [Y[i] for i in test_ids]\n",
    "\n",
    "BATH_SIZE = 10\n",
    "\n",
    "x_batches = list(chunks(encoded_X[\"input_ids\"], BATH_SIZE))\n",
    "y_batches = list(chunks(encoded_Y[\"input_ids\"], BATH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb2cf1c-7c91-4341-8219-16bbebb79294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  Simi¬ larly, the broken line from U to U' is twice\n",
      "output:  Simi¬ larly, the broken line from U to U' is twice as long as the line from U to U' and the line from U to U' is twice as long as the line from U to U' and the line from U to U' is twice as long as the line from U to U' and the line from U to U' is twice as long as the line from U to U' and the line from U to U' is twice as long as the line from U to U' and the line from U to U' is twice\n"
     ]
    }
   ],
   "source": [
    "test_X = [X[i] for i in test_ids]\n",
    "enc_test_X = tokenizer.batch_encode_plus(test_X, \n",
    "                                     add_special_tokens=False, \n",
    "                                     return_tensors='pt', \n",
    "                                     padding=True) # X is encoded Y isnt !\n",
    "                                         \n",
    "output = llm_model.generate(enc_test_X[\"input_ids\"].to(device),  \n",
    "                            max_new_tokens=100, \n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Set padding to the left side as the model is a decoder-only architecture\n",
    "\n",
    "for i, text in enumerate(test_X[0:1]):\n",
    "    print(\"input:\", text) # <<<!\n",
    "    print(\"output:\", tokenizer.decode(output[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33487168-8fbb-4b64-884e-e1eaaaede66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  In particular, the coordinates (X, F, 1) of P itself \n",
      "\n",
      "input_ids: tensor([[  554,  1948,    11,   262, 22715,   357,    55,    11,   376,    11,\n",
      "           352,     8,   286,   350,  2346]], device='cuda:0') \n",
      "\n",
      "output: tensor([[  554,  1948,    11,   262, 22715,   357,    55,    11,   376,    11,\n",
      "           352,     8,   286,   350,  2346,   389,   407,  1900,    13,   383,\n",
      "         22715,   286,   262,   734, 22715,   286,   262,   734, 22715,   286,\n",
      "           262,   734, 22715,   286,   262,   734, 22715,   286,   262,   734,\n",
      "         22715,   286,   262,   734, 22715,   286,   262,   734, 22715,   286,\n",
      "           262,   734, 22715,   286,   262,   734, 22715,   286,   262,   734,\n",
      "         22715,   286,   262,   734, 22715,   286,   262,   734, 22715,   286,\n",
      "           262,   734, 22715,   286,   262,   734, 22715,   286,   262,   734,\n",
      "         22715,   286,   262,   734, 22715,   286,   262,   734, 22715,   286,\n",
      "           262,   734, 22715,   286,   262,   734, 22715,   286,   262,   734,\n",
      "         22715,   286,   262,   734, 22715,   286,   262,   734, 22715,   286,\n",
      "           262,   734, 22715,   286,   262]], device='cuda:0') \n",
      "\n",
      "decoded_output: In particular, the coordinates (X, F, 1) of P itself are not known. The coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the two coordinates of the\n",
      "\n",
      "expected output:are a set of homogeneous coordinates for P \n"
     ]
    }
   ],
   "source": [
    "# Prepare input data\n",
    "\n",
    "tst_i = 3\n",
    "test_str = test_X[tst_i]\n",
    "input_ids = tokenizer.encode(test_str,\n",
    "                             add_special_tokens=False, \n",
    "                             return_tensors='pt', \n",
    "                             padding=True).to(device)\n",
    "\n",
    "# Generate text using generation parameters\n",
    "output = llm_model.generate(input_ids , \n",
    "                            max_new_tokens=100, \n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(output[0])\n",
    "print(f\"input: {test_str} \\n\")\n",
    "print(f\"input_ids: {input_ids} \\n\")\n",
    "print(f\"output: {output} \\n\")\n",
    "\n",
    "print(f\"decoded_output:{decoded_output}\\n\", )\n",
    "print(f\"expected output:{test_Y[tst_i]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae25b5ed-50c6-4337-8134-2bff8862908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(llm_model.parameters(), lr=1e-5)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcbbcac2-39c4-4ec3-9f7c-3ea27a109688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x_batches[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]): \n\u001b[1;32m---> 14\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     16\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:837\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    834\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_layer)\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 837\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[0;32m    839\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "EPOCHS=10\n",
    "\n",
    "mlflow.end_run()\n",
    "gradient_accumulation_steps = 10\n",
    "grad_counter = 0\n",
    "\n",
    "with mlflow.start_run(run_name='final') as run:\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for index, batch in enumerate(x_batches[0:5]): \n",
    "            outputs = llm_model(batch, labels=y_batches[index])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            if grad_counter % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                print(\"batch loss \", loss.item())\n",
    "\n",
    "                \n",
    "            grad_counter += 1\n",
    "                            \n",
    "\n",
    "        # log metric every epoch\n",
    "        mlflow.log_metric('loss', loss.item())\n",
    "        mlflow.log_metric('lr', get_lr(optimizer))\n",
    "\n",
    "        \n",
    "        output = llm_model.generate(tst_data[\"input_ids\"].to(device))\n",
    "        test_output = \"\"\n",
    "        for i in range(output.shape[0]):\n",
    "            tmp_str = tokenizer.decode(output[i])\n",
    "            test_output += \"\\n\" + tmp_str\n",
    "        mlflow.log_text(test_output, f\"test_{epoch}.txt\")\n",
    "        #### \n",
    "\n",
    "        \n",
    "        # Save the fine-tuned model every epoch \n",
    "        print(\"MODEL saved loss \", loss)\n",
    "        llm_model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89cbc93-1cd3-463e-9bb3-472f3dbe7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tst_text = np.random.choice(math_sentences, 30)\n",
    "tst_data = tokenizer.batch_encode_plus(tst_text, add_special_tokens=True, return_tensors='pt', padding=True).to(device)\n",
    "output = llm_model.generate(tst_data[\"input_ids\"])\n",
    "\n",
    "for i, text in enumerate(tst_text):\n",
    "    print(\"input:\", text)\n",
    "    print(\"output\", tokenizer.decode(output[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe6a06-f620-4ed7-9537-026702defb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "      tokenizer=tokenizer,\n",
    "      file_path=math_example_path,\n",
    "      block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6078a30-e05d-447f-9ca4-e0dcd4c38fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    default_data_collator, \n",
    ")\n",
    "import mlflow \n",
    "mlflow.end_run()\n",
    "\n",
    "training_args = TrainingArguments(output_dir='test_trainer', \n",
    "                                  #evaluation_strategy='epoch',\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  gradient_accumulation_steps=20, #\n",
    "                                  num_train_epochs = 2,\n",
    "                                  fp16=False,)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=llm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    #eval_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=None,\n",
    "    preprocess_logits_for_metrics=None,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027628bf-339b-41ad-992a-43f5145a5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381e7ac-d923-47ab-98b5-b355409bf892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
